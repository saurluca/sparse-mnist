# Sparse MNIST Techincal Project

This project investigates whether dendritic artificial neural networks (dANNs), inspired by the work of Chavlis & Poirazi[1], provide measurable advantages over conventional vanilla artificial neural networks (vANNs). Both models were implemented from scratch in Numpy to ensure full control over architecture training dynamics and optimisation. We benchmarked them on the Fashion-MNIST dataset, which poses a more challenging classification problem than MNIST while remaining computationally tractable.  
Our experiments systematically compared optimisers (SGD vs. Adam), activation functions (ReLU, LeakyReLU, Sigmoid), and learning rates. The dendritic model achieved a consistent improvement of ~86.4% test accuracy over the baseline MLP's accuracy of 85.5%, while demonstrating greater robustness across hyperparameters. These results suggest that dendritic architectures offer not only increased parameter efficiency but also improved generalisation. Lessons learned include the importance of modular code design and the challenges of faithfully translating biological principles into efficient deep learning models.
